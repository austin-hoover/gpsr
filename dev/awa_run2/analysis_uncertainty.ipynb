{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\phase_space_reconstruction\\lib\\site-packages\\torch\\functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm emit: 4.9825580390461255e-06\n",
      "norm emit: 4.920143510389607e-06\n",
      "norm emit: 5.011093890061602e-06\n",
      "norm emit: 4.879185780737316e-06\n",
      "norm emit: 5.390817022998817e-06\n",
      "norm emit: 4.7178104978229385e-06\n",
      "norm emit: 4.882823759544408e-06\n",
      "norm emit: 5.253022663964657e-06\n",
      "norm emit: 5.354805125534767e-06\n",
      "norm emit: 4.543398063105997e-06\n",
      "norm emit: 4.2762844714161474e-06\n",
      "norm emit: 4.151869688939769e-06\n",
      "norm emit: 4.312117198423948e-06\n",
      "norm emit: 4.2902870518446434e-06\n",
      "norm emit: 4.150795120949624e-06\n",
      "norm emit: 4.310288659326034e-06\n",
      "norm emit: 4.541454018180957e-06\n",
      "norm emit: 4.4161902224004734e-06\n",
      "norm emit: 3.866092811222188e-06\n",
      "norm emit: 4.507598987402162e-06\n",
      "norm emit: 4.113294835406123e-06\n",
      "norm emit: 3.975808795075864e-06\n",
      "norm emit: 3.989757715316955e-06\n",
      "norm emit: 4.338173766882392e-06\n",
      "norm emit: 4.088330115337158e-06\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fitting_uncertainty import create_ensemble, load_data, create_datasets\n",
    "import torch\n",
    "\n",
    "tkwargs = {\"dtype\": torch.float}\n",
    "scales = [0.90, 0.95, 1.0, 1.05, 1.1]\n",
    "\n",
    "emittances = []\n",
    "betas = []\n",
    "alphas = []\n",
    "\n",
    "for scale in scales:\n",
    "    semits = []\n",
    "    sbetas = []\n",
    "    salphas = []\n",
    "    for ensemble_index in [0,1,2,3,4]:\n",
    "        #print(scale)\n",
    "        tkwargs = {\"dtype\": torch.float}\n",
    "        save_dir = f\"uncertainty/ensemble_mse_scale_{scale}\"\n",
    "        quad_strengths, image_data, bins, xx = load_data(tkwargs)\n",
    "        train_dset = torch.load(\"uncertainty/train.dset\")\n",
    "        test_dset = torch.load(\"uncertainty/test.dset\")\n",
    "\n",
    "        bin_width = bins[1] - bins[0]\n",
    "        bandwidth = bin_width / 2\n",
    "        ensemble = create_ensemble(bins, bandwidth)\n",
    "\n",
    "        from torchensemble.utils import io\n",
    "        io.load(ensemble, save_dir)\n",
    "\n",
    "        n_particles = 100000\n",
    "        #ensemble_index = -1\n",
    "        ensemble[ensemble_index].beam.set_base_beam(\n",
    "            ensemble[ensemble_index].beam.base_dist,\n",
    "            n_particles,\n",
    "            p0c=torch.tensor(63.0e6)\n",
    "        )\n",
    "\n",
    "        ensemble.cuda();\n",
    "\n",
    "        initial_beam = ensemble[ensemble_index].beam()\n",
    "\n",
    "        # propagate particles w/quad off\n",
    "        initial_y = initial_beam.y.cpu().detach().numpy()*1e3\n",
    "        initial_py = initial_beam.py.cpu().detach().numpy()*1e3\n",
    "\n",
    "        # distances\n",
    "        dist_to_slits = 3.38 - 2.84 + 0.12/2.0\n",
    "        dist_to_screen = 3.38 + 0.12/2.0\n",
    "\n",
    "        # particles\n",
    "        slits_y = initial_y + initial_py * dist_to_slits\n",
    "        slits_py = initial_py\n",
    "\n",
    "        screen_y = initial_y + initial_py * dist_to_screen\n",
    "        screen_py = initial_py\n",
    "\n",
    "        cov = torch.cov(initial_beam.data.T)\n",
    "        ycov = cov[2:4,2:4]\n",
    "\n",
    "        emit = torch.det(ycov).sqrt()\n",
    "        twiss = ycov / emit\n",
    "        #print(f\"beta {twiss[0,0]}\")\n",
    "        #print(f\"alpha {-twiss[1,0]}\")\n",
    "        #print(f\"gamma {twiss[1,1]}\")\n",
    "\n",
    "        # geometric emittance\n",
    "        #print(f\"geo_emit: {emit}\")\n",
    "        print(f\"norm emit: {emit*63.0/0.511}\")\n",
    "        \n",
    "        semits.append(emit*63.0/0.511)\n",
    "        sbetas.append(twiss[0,0])\n",
    "        salphas.append(-twiss[1,0])\n",
    "    emittances.append(semits)\n",
    "    betas.append(sbetas)\n",
    "    alphas.append(salphas)\n",
    "\n",
    "emittances = torch.tensor(emittances)\n",
    "betas = torch.tensor(betas)\n",
    "alphas = torch.tensor(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean emit: 4.53056001663208 +/- 0.4461699426174164\n",
      "mean beta: 9.61125373840332 +/- 0.9587207436561584\n",
      "mean alpha: -0.19749179482460022 +/- 0.214345321059227\n"
     ]
    }
   ],
   "source": [
    "# entire stats\n",
    "# calculate propability of each scale if the rms error is 5%\n",
    "dist = torch.distributions.Normal(1.0,0.05)\n",
    "vals = torch.tensor(scales)\n",
    "probs = dist.log_prob(vals).exp().unsqueeze(0).repeat(5, 1).T\n",
    "\n",
    "def get_weighted_stats(vals):\n",
    "    prob_weighted_vals = vals * probs\n",
    "    weighted_val_mean = prob_weighted_vals.sum() / probs.sum()\n",
    "    weighted_val_variance = (probs * (emittances - weighted_val_mean)**2).sum() / probs.sum() \n",
    "    return weighted_val_mean, weighted_val_variance\n",
    "    \n",
    "\n",
    "\n",
    "for name, ele in zip([\"emit\",\"beta\",\"alpha\"],[emittances*1e6, betas, alphas]):\n",
    "    mean, variance = get_weighted_stats(ele)\n",
    "    print(f\"mean {name}: {ele.mean()} +/- {ele.std()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(5):\n",
    "#    # ensemble rms\n",
    "#    for name, ele in zip([\"emit\",\"beta\",\"alpha\"],[emittances[i], betas[i], alphas[i]]):\n",
    "#        ele = torch.tensor(ele)\n",
    "#        print(f\"{i} {name}: {ele.mean()} +/- {ele.std()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.070133209228516 +/- 0.224358469247818\n",
      "8.566444396972656 +/- 0.3918430805206299\n",
      "0.2446666806936264 +/- 0.13221006095409393\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "eny = [4.7797, 5.0640, 5.258, 4.7572, 5.0252, 5.2580, 4.9472, 5.1097, 5.4322] # mm mrad \n",
    "betay = [8.201, 8.630, 9.083, 8.218, 8.644, 9.083, 7.947, 8.517, 8.775] # m\n",
    "alphay = [0.097, 0.240, 0.406, 0.101, 0.248, 0.406, 0.085, 0.234, 0.385] # rad\n",
    "print(f\"{torch.tensor(eny).mean()} +/- {torch.tensor(eny).std()}\")\n",
    "print(f\"{torch.tensor(betay).mean()} +/- {torch.tensor(betay).std()}\")\n",
    "print(f\"{torch.tensor(alphay).mean()} +/- {torch.tensor(alphay).std()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'uncertainty/ensemble_mse_scale_1.0/train.dset'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [5]\u001B[0m, in \u001B[0;36m<cell line: 25>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     29\u001B[0m save_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muncertainty/ensemble_mse_scale_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mscale\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     30\u001B[0m quad_strengths, image_data, bins, xx \u001B[38;5;241m=\u001B[39m load_data(tkwargs)\n\u001B[1;32m---> 31\u001B[0m train_dset \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43msave_dir\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/train.dset\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     32\u001B[0m test_dset \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mload(save_dir \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/test.dset\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     34\u001B[0m bin_width \u001B[38;5;241m=\u001B[39m bins[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m-\u001B[39m bins[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32mC:\\ProgramData\\Miniconda3\\envs\\phase_space_reconstruction\\lib\\site-packages\\torch\\serialization.py:699\u001B[0m, in \u001B[0;36mload\u001B[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001B[0m\n\u001B[0;32m    696\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m    697\u001B[0m     pickle_load_args[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m--> 699\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_file_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[0;32m    700\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[0;32m    701\u001B[0m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[0;32m    702\u001B[0m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[0;32m    703\u001B[0m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[0;32m    704\u001B[0m         orig_position \u001B[38;5;241m=\u001B[39m opened_file\u001B[38;5;241m.\u001B[39mtell()\n",
      "File \u001B[1;32mC:\\ProgramData\\Miniconda3\\envs\\phase_space_reconstruction\\lib\\site-packages\\torch\\serialization.py:231\u001B[0m, in \u001B[0;36m_open_file_like\u001B[1;34m(name_or_buffer, mode)\u001B[0m\n\u001B[0;32m    229\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[0;32m    230\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[1;32m--> 231\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    232\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    233\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "File \u001B[1;32mC:\\ProgramData\\Miniconda3\\envs\\phase_space_reconstruction\\lib\\site-packages\\torch\\serialization.py:212\u001B[0m, in \u001B[0;36m_open_file.__init__\u001B[1;34m(self, name, mode)\u001B[0m\n\u001B[0;32m    211\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[1;32m--> 212\u001B[0m     \u001B[38;5;28msuper\u001B[39m(_open_file, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'uncertainty/ensemble_mse_scale_1.0/train.dset'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "tkwargs = {\"dtype\": torch.float}\n",
    "scales = [1.0]\n",
    "\n",
    "fpath = \"\"\n",
    "\n",
    "meas_y = np.load(fpath + \"y_recon.npy\")*1e3\n",
    "meas_yp = np.load(fpath + \"yp_recon.npy\")*1e3\n",
    "\n",
    "#fig.set_size_inches(10,10)\n",
    "# create a mesh\n",
    "x1 = np.linspace(-1.75,1.75,150)\n",
    "x2 = np.linspace(-0.35,0.35,150)\n",
    "\n",
    "h_slitscan, xe, ye = np.histogram2d(meas_y, meas_yp, bins=[x1,x2], density=True)\n",
    "\n",
    "x1c = (xe[:-1] + xe[1:]) / 2\n",
    "x2c = (ye[:-1] + ye[1:]) / 2\n",
    "\n",
    "X, Y = np.meshgrid(xe,ye)\n",
    "Xc, Yc = np.meshgrid(x1c,x2c)\n",
    "\n",
    "images = []\n",
    "\n",
    "for scale in scales:\n",
    "    for ensemble_index in [0,1,2,3,4]:\n",
    "        #print(scale)\n",
    "        tkwargs = {\"dtype\": torch.float}\n",
    "        save_dir = f\"uncertainty/ensemble_mse_scale_{scale}\"\n",
    "        quad_strengths, image_data, bins, xx = load_data(tkwargs)\n",
    "        train_dset = torch.load(save_dir + \"/train.dset\")\n",
    "        test_dset = torch.load(save_dir + \"/test.dset\")\n",
    "\n",
    "        bin_width = bins[1] - bins[0]\n",
    "        bandwidth = bin_width / 2\n",
    "        ensemble = create_ensemble(bins, bandwidth)\n",
    "\n",
    "        from torchensemble.utils import io\n",
    "        io.load(ensemble, save_dir)\n",
    "\n",
    "        n_particles = 100000\n",
    "        #ensemble_index = -1\n",
    "        ensemble[ensemble_index].beam.set_base_beam(\n",
    "            ensemble[ensemble_index].beam.base_dist,\n",
    "            n_particles,\n",
    "            p0c=torch.tensor(63.0e6)\n",
    "        )\n",
    "\n",
    "        ensemble.cuda();\n",
    "\n",
    "        initial_beam = ensemble[ensemble_index].beam()\n",
    "\n",
    "        # propagate particles w/quad off\n",
    "        initial_y = initial_beam.y.cpu().detach().numpy()*1e3\n",
    "        initial_py = initial_beam.py.cpu().detach().numpy()*1e3\n",
    "\n",
    "        # distances\n",
    "        dist_to_slits = 3.38 - 2.84 + 0.12/2.0\n",
    "        dist_to_screen = 3.38 + 0.12/2.0\n",
    "\n",
    "        # particles\n",
    "        slits_y = initial_y + initial_py * dist_to_slits\n",
    "        slits_py = initial_py\n",
    "\n",
    "        screen_y = initial_y + initial_py * dist_to_screen\n",
    "        screen_py = initial_py\n",
    "\n",
    "        h_reconstruction, xe, ye = np.histogram2d(slits_y, slits_py, bins=[x1,x2], density=True)\n",
    "        images.append(h_reconstruction)\n",
    "\n",
    "images = np.stack(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_images = images.mean(axis=0)\n",
    "std_images = images.std(axis=0)\n",
    "\n",
    "\n",
    "plt.imshow(mean_images.T,vmax=5)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.imshow(std_images.T,vmax=5)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
