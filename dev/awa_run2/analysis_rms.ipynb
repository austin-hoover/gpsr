{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/rroussel/.conda/envs/phase_space_reconstruction/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/global/homes/r/rroussel/.conda/envs/phase_space_reconstruction/lib/python3.10/site-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755897462/work/aten/src/ATen/native/TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from fitting import create_ensemble, load_data, create_datasets\n",
    "import torch\n",
    "labels = [\"x (mm)\", r\"$p_x$ (mrad)\",\"y (mm)\",r\"$p_y$ (mrad)\",\"z (mm)\",r\"$p_z$ (mrad)\"]\n",
    "\n",
    "tkwargs = {\"dtype\": torch.float}\n",
    "base_dir = \"/global/cfs/cdirs/m669/rroussel/phase_space_reconstruction\"\n",
    "save_dir = base_dir + \"/mse_scale_1_l_1e10\"\n",
    "quad_strengths, image_data, bins, xx = load_data(base_dir, tkwargs)\n",
    "xx = xx.cpu()\n",
    "train_dset = torch.load(save_dir + \"/train.dset\")\n",
    "test_dset = torch.load(save_dir + \"/test.dset\")\n",
    "\n",
    "bin_width = bins[1] - bins[0]\n",
    "bandwidth = bin_width / 2\n",
    "ensemble = create_ensemble(bins, bandwidth)\n",
    "\n",
    "from torchensemble.utils import io\n",
    "io.load(ensemble, save_dir)\n",
    "\n",
    "n_particles = 1000000\n",
    "for ele in ensemble:\n",
    "    ele.beam.set_base_beam(\n",
    "        ele.beam.base_dist,\n",
    "        n_particles,\n",
    "        p0c=torch.tensor(63.0e6)\n",
    "    )\n",
    "\n",
    "ensemble = ensemble.cuda();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 9.31 GiB (GPU 0; 39.45 GiB total capacity; 28.15 GiB already allocated; 9.30 GiB free; 28.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m en_covs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(quad_strengths)):\n\u001b[0;32m---> 22\u001b[0m     p, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mensemble\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquad_strengths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     p \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclip(p\u001b[38;5;241m.\u001b[39mcpu(), offset) \u001b[38;5;241m-\u001b[39m offset\n\u001b[1;32m     24\u001b[0m     centroid, cov \u001b[38;5;241m=\u001b[39m calculate_ellipse(p\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), bins, bins)\n",
      "File \u001b[0;32m~/.conda/envs/phase_space_reconstruction/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/global/u1/r/rroussel/phase_space_reconstruction/phase_space_reconstruction/modeling.py:40\u001b[0m, in \u001b[0;36mPhaseSpaceReconstructionModel.forward\u001b[0;34m(self, K)\u001b[0m\n\u001b[1;32m     37\u001b[0m proposal_beam \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeam()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# track beam\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m observations, final_beam \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_and_observe_beam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproposal_beam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# get entropy\u001b[39;00m\n\u001b[1;32m     43\u001b[0m entropy \u001b[38;5;241m=\u001b[39m calculate_beam_entropy(proposal_beam)\n",
      "File \u001b[0;32m/global/u1/r/rroussel/phase_space_reconstruction/phase_space_reconstruction/modeling.py:32\u001b[0m, in \u001b[0;36mPhaseSpaceReconstructionModel.track_and_observe_beam\u001b[0;34m(self, beam, K)\u001b[0m\n\u001b[1;32m     29\u001b[0m final_beam \u001b[38;5;241m=\u001b[39m lattice(beam)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# analyze beam with diagnostic\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m observations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiagnostic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_beam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m observations, final_beam\n",
      "File \u001b[0;32m~/.conda/envs/phase_space_reconstruction/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/global/u1/r/rroussel/phase_space_reconstruction/phase_space_reconstruction/diagnostics.py:45\u001b[0m, in \u001b[0;36mImageDiagnostic.forward\u001b[0;34m(self, beam)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x_vals\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoords must be at least 2D\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhistogram2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbandwidth\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/u1/r/rroussel/phase_space_reconstruction/phase_space_reconstruction/histogram.py:206\u001b[0m, in \u001b[0;36mhistogram2d\u001b[0;34m(x1, x2, bins, bandwidth, epsilon)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m\"\"\"Estimate the 2d histogram of the input tensor.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03mThe calculation uses kernel density estimation which requires a bandwidth (smoothing) parameter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m    torch.Size([2, 128, 128])\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m _, kernel_values1 \u001b[38;5;241m=\u001b[39m marginal_pdf(x1\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), bins, bandwidth, epsilon)\n\u001b[0;32m--> 206\u001b[0m _, kernel_values2 \u001b[38;5;241m=\u001b[39m \u001b[43mmarginal_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbandwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m pdf \u001b[38;5;241m=\u001b[39m joint_pdf(kernel_values1, kernel_values2)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pdf\n",
      "File \u001b[0;32m/global/u1/r/rroussel/phase_space_reconstruction/phase_space_reconstruction/histogram.py:94\u001b[0m, in \u001b[0;36mmarginal_pdf\u001b[0;34m(values, bins, sigma, epsilon)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput sigma must be a of the shape 1\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(sigma\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     91\u001b[0m     )\n\u001b[1;32m     93\u001b[0m residuals \u001b[38;5;241m=\u001b[39m values \u001b[38;5;241m-\u001b[39m bins\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m*\u001b[39mvalues\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 94\u001b[0m kernel_values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[43m(\u001b[49m\u001b[43mresiduals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     96\u001b[0m pdf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(kernel_values, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     97\u001b[0m normalization \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(pdf, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m epsilon\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 9.31 GiB (GPU 0; 39.45 GiB total capacity; 28.15 GiB already allocated; 9.30 GiB free; 28.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from phase_space_reconstruction.utils import calculate_ellipse\n",
    "offset = 0\n",
    "# calculate rms stats from each image\n",
    "covs = []\n",
    "centroids = []\n",
    "image_data = torch.clip(image_data, offset) - offset\n",
    "for ele in image_data.transpose(-1,-2):\n",
    "    centroid, cov = calculate_ellipse(ele, bins, bins)\n",
    "    covs += [cov]\n",
    "    centroids += [centroid]\n",
    "\n",
    "covs = torch.stack(covs).sqrt().detach().cpu()\n",
    "centroids = torch.stack(centroids)\n",
    "\n",
    "# plot reconstruction covs\n",
    "rcovs = []\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for i in range(len(ensemble)):\n",
    "        en_covs = []\n",
    "        for j in range(len(quad_strengths)):\n",
    "            p, _, _ = ensemble[i](quad_strengths[j,:,:].cuda())\n",
    "            p = torch.clip(p.cpu(), offset) - offset\n",
    "            centroid, cov = calculate_ellipse(p.transpose(-2,-1), bins, bins)\n",
    "            en_covs += [cov]\n",
    "            \n",
    "            if i==0:\n",
    "                predictions += [p]\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        rcovs += [torch.stack(en_covs).squeeze()]\n",
    "rcovs = torch.stack(rcovs).sqrt().transpose(0,1)\n",
    "rcovs_mean = rcovs.mean(dim=[1,2])\n",
    "rcovs_std = rcovs.std(dim=[1,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(covs.shape)\n",
    "print(rcovs.shape)\n",
    "print(predictions[0].shape)\n",
    "print(bin_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "indicies = np.arange(-5,-1)\n",
    "\n",
    "fig,ax = plt.subplots(len(indicies),3,sharex=\"all\", sharey=\"all\")\n",
    "fig.set_size_inches(20,30)\n",
    "shot_index = 0\n",
    "\n",
    "vmax = 0.001\n",
    "for i, index in enumerate(indicies):\n",
    "    ax[i][0].pcolor(*xx, \n",
    "                    image_data[index][shot_index].cpu().detach(),\n",
    "                    vmin=0,vmax=vmax\n",
    "                    )\n",
    "    ax[i][1].pcolor(*xx, \n",
    "                    predictions[index][shot_index].cpu().detach(), \n",
    "                    vmin=0,vmax=vmax)\n",
    "\n",
    "    c = ax[i][2].pcolor(*xx,image_data[index].mean(dim=0).cpu().detach() - predictions[index][shot_index].cpu()\n",
    "                    .detach(),cmap=\"coolwarm\",vmin=-vmax*0.1, vmax=vmax*0.1)\n",
    "    fig.colorbar(c, ax=ax[i][2])\n",
    "    axb = ax[i][0].twinx()\n",
    "    axb.plot(bins, image_data[index][shot_index].sum(dim=-2))\n",
    "    axb.plot(bins, predictions[index][shot_index].sum(dim=-2))\n",
    "    axb.set_ylim(0.0, image_data[index][shot_index].sum(dim=-2).max()*3)\n",
    "    \n",
    "    axc = ax[i][0].twiny()\n",
    "    axc.plot(image_data[index][shot_index].sum(dim=-1),bins)\n",
    "    axc.plot(predictions[index][shot_index].sum(dim=-1),bins)\n",
    "    axc.set_xlim(0.0, image_data[index][shot_index].sum(dim=-1).max()*3)\n",
    "    \n",
    "    axb.set_title(f\"{covs[index,shot_index,1,1]:.2} - {rcovs[index,shot_index,0,1,1]:.2}\")\n",
    "    ax[i][1].set_title(f\"{covs[index,shot_index,0,0]:.2} - {rcovs[index,shot_index,0,0,0]:.2}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(image_data[index][shot_index].sum(dim=-1))\n",
    "plt.plot(predictions[index][shot_index].sum(dim=-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "slices = [[0,0],[1,1]]\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "for ele in slices:\n",
    "    ax.plot(\n",
    "        quad_strengths.detach().cpu().flatten(),\n",
    "        covs[...,ele[0],ele[1]].flatten()*1e3,\"o\"\n",
    "    )\n",
    "    ax.errorbar(\n",
    "        quad_strengths[:,0,:].detach().cpu().flatten(),\n",
    "        rcovs_mean[...,ele[0],ele[1]].flatten()*1e3,\n",
    "        rcovs_std[...,ele[0],ele[1]].flatten()*1e3\n",
    "    )\n",
    "\n",
    "from phase_space_reconstruction.modeling import NormalizedQuadScan\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "\n",
    "# fit phase spaces\n",
    "train_s11 = covs[...,0,0].detach().cpu().flatten()**2\n",
    "train_s22 = covs[...,1,1].detach().cpu().flatten()**2\n",
    "\n",
    "for ele in [train_s11, train_s22]:\n",
    "    train_k = quad_strengths.flatten().cpu()\n",
    "    quad_length = torch.tensor(0.12)\n",
    "    drift = torch.tensor(3.38 - 0.12/2)\n",
    "    A = train_s11.max().sqrt()\n",
    "\n",
    "    model = NormalizedQuadScan(A, drift, quad_length)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "            model.parameters(), lr=0.01\n",
    "    )\n",
    "\n",
    "    if 1:\n",
    "        for i in range(25000):\n",
    "            # Zero gradients from previous iteration\n",
    "            optimizer.zero_grad()\n",
    "            # Output from model\n",
    "            output = model(train_k)\n",
    "            # Calc loss and backprop gradients\n",
    "            loss = torch.abs(output*1e6 - ele*1e6).mean()\n",
    "            loss.backward()\n",
    "            if not i % 2000:\n",
    "                print(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "        print(list(model.named_parameters()))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_y = model(train_k)\n",
    "        ax.plot(train_k, pred_y.detach().sqrt()*1e3,'--')\n",
    "    print(model.emittance() * 63.0 / 0.511)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    plt.plot(quad_strengths.detach().cpu().flatten(), centroids[...,i].detach().cpu().flatten(),\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from phase_space_reconstruction.modeling import NormalizedQuadScan\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "\n",
    "# fit phase spaces\n",
    "train_s11 = covs[...,0,0].detach().cpu().flatten()**2\n",
    "train_s22 = covs[...,1,1].detach().cpu().flatten()**2\n",
    "\n",
    "for ele in [train_s11, train_s22]:\n",
    "    train_k = quad_strengths.flatten().cpu()\n",
    "    quad_length = torch.tensor(0.12)\n",
    "    drift = torch.tensor(3.38 - 0.12/2)\n",
    "    A = train_s11.max().sqrt()\n",
    "\n",
    "    model = NormalizedQuadScan(A, drift, quad_length)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "            model.parameters(), lr=0.01\n",
    "    )\n",
    "\n",
    "    if 1:\n",
    "        for i in range(25000):\n",
    "            # Zero gradients from previous iteration\n",
    "            optimizer.zero_grad()\n",
    "            # Output from model\n",
    "            output = model(train_k)\n",
    "            # Calc loss and backprop gradients\n",
    "            loss = torch.abs(output*1e6 - ele*1e6).mean()\n",
    "            loss.backward()\n",
    "            if not i % 2000:\n",
    "                print(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "        print(list(model.named_parameters()))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_y = model(train_k)\n",
    "        plt.figure()\n",
    "        plt.plot(train_k, ele*1e6, \"o\")\n",
    "        plt.plot(train_k, pred_y.detach()*1e6)\n",
    "    print(model.emittance() * 63.0 / 0.511)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# do bayesian linear regression using pyro\n",
    "import pyro\n",
    "from pyro import poutine\n",
    "\n",
    "from pyro.nn import PyroModule, PyroParam, PyroSample\n",
    "import pyro.distributions as dist\n",
    "\n",
    "from pyro.infer import Predictive\n",
    "from pyro.infer.autoguide import AutoNormal\n",
    "\n",
    "class PyroNormalizedQuadScan(NormalizedQuadScan, PyroModule):\n",
    "    def forward(self, k, y=None):\n",
    "        sigma = pyro.sample(\"sigma\", dist.Uniform(0., 0.05))\n",
    "        mean = super().forward(k)*1e6\n",
    "        with pyro.plate(\"data\", k.shape[0]):\n",
    "            obs = pyro.sample(\"obs\", dist.Normal(mean, sigma), obs=y)\n",
    "        return mean\n",
    "\n",
    "\n",
    "\n",
    "model = PyroNormalizedQuadScan(A, drift, quad_length)\n",
    "model.lambda_1 = PyroSample(dist.Normal(1.0,5.0))\n",
    "model.lambda_2 = PyroSample(dist.Normal(1.0,5.0))\n",
    "model.c = PyroSample(dist.Normal(0.0,5.0))\n",
    "\n",
    "posterior_module = pyro.nn.PyroModule(\"model\")\n",
    "posterior_module.guide = AutoNormal(poutine.block(model, hide=['bm']))\n",
    "\n",
    "\n",
    "print(dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pyro.infer import SVI, Trace_ELBO\n",
    "\n",
    "def train(model, guide, *args, lr=0.001, n_steps=201, verbose=False):\n",
    "    pyro.clear_param_store()\n",
    "    initial_lr = lr\n",
    "    gamma = 0.1  # final learning rate will be gamma * initial_lr\n",
    "    lrd = gamma ** (1 / n_steps)\n",
    "    optim = pyro.optim.ClippedAdam({'lr': initial_lr, 'lrd': lrd})\n",
    "    svi = SVI(model, guide, optim, loss=Trace_ELBO(num_particles=1))\n",
    "\n",
    "    losses = []\n",
    "    for step in range(n_steps):\n",
    "        loss = svi.step(*args)\n",
    "        losses.append(loss)\n",
    "        if step % 50 == 0 and verbose:\n",
    "            print('[iter {}]  loss: {:.4f}'.format(step, loss))\n",
    "\n",
    "    return losses\n",
    "\n",
    "losses= train(\n",
    "    model,posterior_module.guide, train_k, train_s22*1e6,\n",
    "    lr=0.01,n_steps=4000, verbose=True\n",
    ")\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "posterior_predictive = Predictive(model, num_samples=800, parallel=True,\n",
    "                               guide=posterior_module.guide)\n",
    "test_k = torch.linspace(train_k.min(),train_k.max(),100)\n",
    "\n",
    "posterior_samples = posterior_predictive(test_k)\n",
    "\n",
    "def get_stats(samples):\n",
    "    mean = torch.mean(samples, dim=0)\n",
    "    l = torch.quantile(samples, 0.05, dim=0)\n",
    "    u = torch.quantile(samples, 0.95, dim=0)\n",
    "    return mean, l, u\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "m, l, u = get_stats(posterior_samples[\"obs\"].squeeze()/1e6)\n",
    "ax.plot(test_k.squeeze().cpu(), m.cpu())\n",
    "ax.fill_between(test_k.squeeze().cpu(), l.cpu(), u.cpu(), alpha=0.25)\n",
    "\n",
    "ax.plot(train_k.squeeze().cpu(), train_s22.squeeze().cpu(), '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "emittances = []\n",
    "for i in range(10000):\n",
    "    emittances+=[model.emittance()]\n",
    "emittances = torch.tensor(emittances)\n",
    "qs = torch.quantile(emittances, torch.tensor([0.05,0.5-0.34,0.5,0.5+0.34,0.95]))\n",
    "print(qs)\n",
    "plt.hist(emittances,bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
