{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SnapshotEnsembleRegressor:\n\tMissing key(s) in state_dict: \"estimators_.0.beam.transformer.stack.11.weight\", \"estimators_.0.beam.transformer.stack.11.bias\", \"estimators_.0.beam.transformer.stack.14.weight\", \"estimators_.0.beam.transformer.stack.14.bias\". \n\tsize mismatch for estimators_.0.beam.transformer.stack.8.weight: copying a param with shape torch.Size([6, 20]) from checkpoint, the shape in current model is torch.Size([20, 20]).\n\tsize mismatch for estimators_.0.beam.transformer.stack.8.bias: copying a param with shape torch.Size([6]) from checkpoint, the shape in current model is torch.Size([20]).\n\tsize mismatch for estimators_.0.beam.base_beam.data: copying a param with shape torch.Size([20000, 6]) from checkpoint, the shape in current model is torch.Size([100000, 6]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m ensemble \u001b[38;5;241m=\u001b[39m create_ensemble(bins, bandwidth)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchensemble\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m io\n\u001b[0;32m---> 17\u001b[0m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mensemble\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m n_particles \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20000\u001b[39m\n\u001b[1;32m     20\u001b[0m ensemble[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbeam\u001b[38;5;241m.\u001b[39mset_base_beam(\n\u001b[1;32m     21\u001b[0m     ensemble[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbeam\u001b[38;5;241m.\u001b[39mbase_dist,\n\u001b[1;32m     22\u001b[0m     n_particles,\n\u001b[1;32m     23\u001b[0m     p0c\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m63.0e6\u001b[39m)\n\u001b[1;32m     24\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/phase_space_reconstruction/lib/python3.10/site-packages/torchensemble/utils/io.py:73\u001b[0m, in \u001b[0;36mload\u001b[0;34m(model, save_dir, logger)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_estimators):\n\u001b[1;32m     72\u001b[0m     model\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mappend(model\u001b[38;5;241m.\u001b[39m_make_estimator())\n\u001b[0;32m---> 73\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/phase_space_reconstruction/lib/python3.10/site-packages/torch/nn/modules/module.py:1497\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1492\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   1493\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1494\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1497\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1498\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SnapshotEnsembleRegressor:\n\tMissing key(s) in state_dict: \"estimators_.0.beam.transformer.stack.11.weight\", \"estimators_.0.beam.transformer.stack.11.bias\", \"estimators_.0.beam.transformer.stack.14.weight\", \"estimators_.0.beam.transformer.stack.14.bias\". \n\tsize mismatch for estimators_.0.beam.transformer.stack.8.weight: copying a param with shape torch.Size([6, 20]) from checkpoint, the shape in current model is torch.Size([20, 20]).\n\tsize mismatch for estimators_.0.beam.transformer.stack.8.bias: copying a param with shape torch.Size([6]) from checkpoint, the shape in current model is torch.Size([20]).\n\tsize mismatch for estimators_.0.beam.base_beam.data: copying a param with shape torch.Size([20000, 6]) from checkpoint, the shape in current model is torch.Size([100000, 6])."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fitting import create_ensemble, load_data, create_datasets\n",
    "import torch\n",
    "\n",
    "tkwargs = {\"dtype\": torch.float}\n",
    "save_dir = \"uncertainty/mse_scale_0.95\"\n",
    "quad_strengths, image_data, bins, xx = load_data(tkwargs)\n",
    "train_dset = torch.load(save_dir + \"/train.dset\")\n",
    "test_dset = torch.load(save_dir + \"/test.dset\")\n",
    "\n",
    "bin_width = bins[1] - bins[0]\n",
    "bandwidth = bin_width / 2\n",
    "ensemble = create_ensemble(bins, bandwidth)\n",
    "\n",
    "from torchensemble.utils import io\n",
    "io.load(ensemble, save_dir)\n",
    "\n",
    "n_particles = 20000\n",
    "ensemble[0].beam.set_base_beam(\n",
    "    ensemble[0].beam.base_dist,\n",
    "    n_particles,\n",
    "    p0c=torch.tensor(63.0e6)\n",
    ")\n",
    "\n",
    "ensemble.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "s = torch.arange(0,11,2)\n",
    "with torch.no_grad():\n",
    "    predictions, entropy = ensemble[0](train_dset[s][0].cuda())\n",
    "\n",
    "fig,ax = plt.subplots(len(predictions),2,sharex=\"all\", sharey=\"all\")\n",
    "fig.set_size_inches(10,30)\n",
    "for i in range(len(s)):\n",
    "    ax[i][0].pcolor(*xx, train_dset[s[i]][1][0].cpu().detach(),\n",
    "                    vmin=0,vmax=0.01\n",
    "                    )\n",
    "    ax[i][1].pcolor(*xx,predictions[i][0].cpu().detach(), vmin=0,vmax=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "s = torch.arange(0,10,2)\n",
    "with torch.no_grad():\n",
    "    predictions, entropy = ensemble[0](test_dset[s][0].cuda())\n",
    "\n",
    "fig,ax = plt.subplots(len(predictions),2,sharex=\"all\", sharey=\"all\")\n",
    "fig.set_size_inches(10,30)\n",
    "for i in range(len(s)):\n",
    "    ax[i][0].pcolor(*xx,test_dset[s[i]][1][0].cpu().detach(),\n",
    "                    vmin=0,vmax=0.01\n",
    "                    )\n",
    "    ax[i][1].pcolor(*xx,predictions[i][0].cpu().detach(), vmin=0,vmax=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# zero quad strength\n",
    "with torch.no_grad():\n",
    "    predictions, entropy = ensemble[0](torch.zeros(1,1).cuda())\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "fig.set_size_inches(3,3)\n",
    "ax.pcolor(*xx,predictions[0].cpu().detach(), vmin=0,vmax=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# plot loss record\n",
    "loss = torch.stack(torch.load(save_dir + \"/loss_log.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.semilogy(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "initial_beam = ensemble[0].beam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# propagate particles w/quad off\n",
    "initial_y = initial_beam.y.cpu().detach().numpy()*1e3\n",
    "initial_py = initial_beam.py.cpu().detach().numpy()*1e3\n",
    "\n",
    "# distances\n",
    "dist_to_slits = 3.38 - 2.84 + 0.12/2.0\n",
    "dist_to_screen = 3.38 + 0.12/2.0\n",
    "\n",
    "# particles\n",
    "slits_y = initial_y + initial_py * dist_to_slits\n",
    "slits_py = initial_py\n",
    "\n",
    "screen_y = initial_y + initial_py * dist_to_screen\n",
    "screen_py = initial_py\n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "ax.plot(\n",
    "    initial_y,\n",
    "    initial_py,\n",
    "    '.',ms=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "ax.plot(\n",
    "    slits_y[::100], slits_py[::100],\n",
    "    '.',ms=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "import numpy as np\n",
    "# create a mesh\n",
    "x = np.linspace(-3,3,200)\n",
    "y = np.linspace(-0.35,0.35,200)\n",
    "\n",
    "h, xe, ye = np.histogram2d(slits_y, slits_py, bins=[x,y])\n",
    "#xc = (xe[:-1] + xe[1:]) / 2\n",
    "#yc = (ye[:-1] + ye[1:]) / 2\n",
    "X, Y = np.meshgrid(xe,ye)\n",
    "\n",
    "ax.pcolor(X, Y, h.T)\n",
    "ax.set_ylabel(r\"$p_y$ (mrad)\")\n",
    "ax.set_xlabel(r\"$y$ (mm)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "ax.hist(\n",
    "    slits_y, bins=100\n",
    ");\n",
    "ax.set_xlabel(r\"$y$ (mm)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "ax.hist(\n",
    "    slits_py, bins=100\n",
    ");\n",
    "ax.set_xlabel(r\"$yp$ (mrad)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cov = torch.cov(initial_beam.data.T)\n",
    "cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ycov = cov[2:4,2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.transforms as transforms\n",
    "def confidence_ellipse(cov, n_std=3.0, facecolor='none', **kwargs):\n",
    "    \"\"\"\n",
    "    Create a plot of the covariance confidence ellipse of *x* and *y*.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y : array-like, shape (n, )\n",
    "        Input data.\n",
    "\n",
    "    ax : matplotlib.axes.Axes\n",
    "        The axes object to draw the ellipse into.\n",
    "\n",
    "    n_std : float\n",
    "        The number of standard deviations to determine the ellipse's radiuses.\n",
    "\n",
    "    **kwargs\n",
    "        Forwarded to `~matplotlib.patches.Ellipse`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.patches.Ellipse\n",
    "    \"\"\"\n",
    "\n",
    "    pearson = cov[0, 1]/np.sqrt(cov[0, 0] * cov[1, 1])\n",
    "    # Using a special case to obtain the eigenvalues of this\n",
    "    # two-dimensional dataset.\n",
    "    ell_radius_x = np.sqrt(1 + pearson)\n",
    "    ell_radius_y = np.sqrt(1 - pearson)\n",
    "    ellipse = Ellipse((0, 0), width=ell_radius_x * 2, height=ell_radius_y * 2,\n",
    "                      facecolor=facecolor, **kwargs)\n",
    "\n",
    "    # Calculating the standard deviation of x from\n",
    "    # the squareroot of the variance and multiplying\n",
    "    # with the given number of standard deviations.\n",
    "    scale_x = np.sqrt(cov[0, 0]) * n_std\n",
    "    mean_x = np.mean(x)\n",
    "\n",
    "    # calculating the standard deviation of y ...\n",
    "    scale_y = np.sqrt(cov[1, 1]) * n_std\n",
    "    mean_y = np.mean(y)\n",
    "\n",
    "    transf = transforms.Affine2D() \\\n",
    "        .rotate_deg(45) \\\n",
    "        .scale(scale_x, scale_y) \\\n",
    "        .translate(mean_x, mean_y)\n",
    "\n",
    "    ellipse.set_transform(transf + ax.transData)\n",
    "    return ellipse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "emit = torch.det(ycov).sqrt()\n",
    "twiss = ycov / emit\n",
    "print(f\"emit {emit}\")\n",
    "print(f\"beta {twiss[0,0]}\")\n",
    "print(f\"alpha {-twiss[1,0]}\")\n",
    "print(f\"gamma {twiss[1,1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# geometric emittance\n",
    "print(f\"geo_emit: {emit}\")\n",
    "print(f\"norm emit: {emit*63.0/0.511}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# compare reconstruction to slit scan measurement\n",
    "import numpy as np\n",
    "from scipy.ndimage import median_filter\n",
    "\n",
    "fpath = \"\"\n",
    "\n",
    "meas_y = np.load(fpath + \"y_recon.npy\")*1e3\n",
    "meas_yp = np.load(fpath + \"yp_recon.npy\")*1e3\n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "#fig.set_size_inches(10,10)\n",
    "# create a mesh\n",
    "x1 = np.linspace(-1.75,1.75,150)\n",
    "x2 = np.linspace(-0.35,0.35,150)\n",
    "\n",
    "h_reconstruction, xe, ye = np.histogram2d(slits_y, slits_py, bins=[x1,x2], density=True)\n",
    "h_slitscan, _, _ = np.histogram2d(meas_y, meas_yp, bins=[x1,x2], density=True)\n",
    "\n",
    "# apply median filter\n",
    "h_reconstruction = median_filter(h_reconstruction, 3)\n",
    "h_slitscan = median_filter(h_slitscan,3)\n",
    "\n",
    "x1c = (xe[:-1] + xe[1:]) / 2\n",
    "x2c = (ye[:-1] + ye[1:]) / 2\n",
    "\n",
    "X, Y = np.meshgrid(xe,ye)\n",
    "Xc, Yc = np.meshgrid(x1c,x2c)\n",
    "\n",
    "c1 = ax.pcolor(X, Y, h_reconstruction.T)\n",
    "#ax.contour(Xc, Yc, h_reconstruction.T,levels=3)\n",
    "clevels = np.arange(0,5)\n",
    "c2 = ax.contour(Xc, Yc, h_slitscan.T, levels=clevels,cmap=\"Greys\")\n",
    "\n",
    "cbar = fig.colorbar(c1)\n",
    "cbar.set_ticks(clevels)\n",
    "cbar.add_lines(c2)\n",
    "# add ellipse\n",
    "#ax.add_patch(confidence_ellipse(ycov.cpu().detach().numpy(),1.0e3,edgecolor='red'))\n",
    "\n",
    "ax.set_ylabel(r\"$p_y$ (mrad)\")\n",
    "ax.set_xlabel(r\"$y$ (mm)\")\n",
    "ax.set_aspect(5)\n",
    "fig.savefig(f\"reconstruction_comparison_{save_dir}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# do core analysis\n",
    "initial_data = initial_beam.data[:,2:4].cpu()\n",
    "\n",
    "# sort by distance from origin in 4D phase space\n",
    "initial_norm = torch.norm(initial_data, dim=-1)\n",
    "initial_data_sorted = initial_data[torch.argsort(initial_norm),:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# get cov/emit of 95%\n",
    "frac = 0.90\n",
    "partial = initial_data_sorted[:int(initial_data_sorted.shape[0]*frac)]\n",
    "cov = torch.cov(partial.T)\n",
    "\n",
    "plt.plot(torch.norm(initial_data_sorted, dim=-1).detach().numpy())\n",
    "plt.plot(torch.norm(partial, dim=-1).detach().numpy())\n",
    "\n",
    "emit = torch.det(cov).sqrt()\n",
    "print(f\"geo_emit: {emit}\")\n",
    "print(f\"norm emit: {emit*63.0/0.511}\")\n",
    "\n",
    "twiss = cov / emit\n",
    "print(f\"beta {twiss[0,0]}\")\n",
    "print(f\"alpha {-twiss[1,0]}\")\n",
    "print(f\"gamma {twiss[1,1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
